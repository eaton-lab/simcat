#!/usr/bin/env python

"""
Machine learning code for analyzing database info...
"""

import os

import toytree
import toyplot
import h5py
import numpy as np
import pandas as pd

from sklearn.manifold import TSNE
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.decomposition import NMF
from sklearn.model_selection import train_test_split
# from sklearn.metrics import confusion_matrix

from .utils import get_all_admix_edges
from .Model import Model

"""
## Ideas 
1. Visualization and feature extraction: tSNE (base PCA or NMF)
2. Visualization of features: NMF or ExtraTrees.
3. Classification: ExtraTrees using raw or extracted features.
4. Classification + linear fit (e.g., supervised with edges and props.)

5. For a given true tree, we can map onto it which edges CAN be inferred and 
which are unlikely to be estimated correctly even with perfect data. 

6. 
"""


class Analysis(object):
    """
    ...

    Parameters
    ----------
    name: (str)
        Name of Database object used a prefix for database file.
    workdir: (str)
        Path name to directory where database files are located.
    scale: (int)
        0=None, 1=Normalize nxnx16x16 matrix, 2=normalize each 16x16 matrix.
    ulabel: (bool)
        Use undirected labels
    quiet: (bool)
        print progress
    seed: (int)
        random seed
    """
    def __init__(
            self,
            name, 
            workdir=os.path.abspath("./"), 
            scale=1, 
            ulabel=False, 
            transform=False,
            quiet=False,
            seed=0,
            run=True,
            ):

        # store arguments
        self.name = name
        self.workdir = workdir
        self.db_counts = os.path.join(self.workdir, self.name + ".counts.h5")
        self.db_labels = os.path.join(self.workdir, self.name + ".labels.h5")
        self.scale = scale
        self.ulabel = ulabel
        self.transform = transform
        self.quiet = quiet
        self.seed = seed

        # fill X and y with load_database()
        self.tree = None
        self.counts = None
        self.df = None
        self.X = None
        self.y = None
        self.load_database()

        # transform data to reduce dimensionality / extract features
        self.transform_data()

        # fill test/train with split dataset 
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.train_test_split()

        # train a supervised learning model
        self.model = None
        if run:
            self.train_model()


    def load_database(self):
        """
        Load counts and labels Hdf5 databases generated by simcat.Database.
        """
        with h5py.File(self.db_counts) as io5:
            self.tree = toytree.tree(io5.attrs["tree"])
            self.counts = io5["counts"][:]

            if self.scale == 1:
                self.counts = self.counts / self.counts.max()
                self.X = np.array([i.flatten() for i in self.counts])

            if self.scale == 2:
                # iterate over matrixsets
                for i in range(self.counts.shape[0]):
                    # iterate over matrices
                    for j in range(self.counts.shape[1]):
                        # norm 16x16 matrices
                        self.counts[i, j] = (
                            self.counts[i, j] / self.counts[i, j].max()
                        )

            # flatten counts into X
            self.X = np.array([i.flatten() for i in self.counts])


        with h5py.File(self.db_labels) as io5:
            df = pd.DataFrame({
                "theta": io5["thetas"][:],
                "asource": io5["admix_sources"][:].flatten(),
                "atarget": io5["admix_targets"][:].flatten(),
                "atimes": io5["admix_times"][:].flatten(),
                "aprops": io5["admix_props"][:].flatten(),
            })
            
            # make a category for each (source, target) pair
            df["label"] = (
                df.asource.astype(str).str.cat(
                    df.atarget.astype(str), sep=",")
                )

            # unordered labels (no directionality)
            df["ulabel"] = ["{},{}".format(*i) for i in df.label.apply(
                lambda x: sorted([int(i) for i in x.split(",")]))]

            # add a column to dataframe for sisters migration or not
            df["sisters"] = 0
            for idx in df.index:
                sis1 = df.asource[idx]
                sis2 = df.atarget[idx]
                node1 = self.tree.treenode.search_nodes(idx=sis1)[0]
                node2 = self.tree.treenode.search_nodes(idx=sis2)[0]
                if node1 in node2.get_sisters():
                    df.loc[idx, "sisters"] = 1
            self.df = df

        # select to use directed or undirected labels
        if self.ulabel:
            self.y = self.df.ulabel
        else:
            self.y = self.df.label

        # print to screen
        if not self.quiet:
            print("Dataset: {}".format(self.name))
            print("loaded counts matrix: {}".format(self.counts.shape))
            if self.scale == 1:
                print("scaled integers to floats by max count")
            if self.scale == 2:
                print("scaled integers to floats by each quartet max count")
            print("reshaped into X: {}".format(self.X.shape))
            print("loaded labels DataFrame: {}".format(self.df.shape))
            print("subset as y: {}".format(self.y.shape))        



    def transform_data(self):
        """
        Use a dimensionality reduction method.
        """
        if not self.transform:
            return 


        # NMF n_components can be nfeatures-1 but slow
        if self.transform in ("nmf", "NMF"):
            if not self.quiet:
                print("training NMF model for data tranform")
            nmf = NMF(
                n_components=50, 
                init="random", 
                random_state=0)
            self.X = nmf.fit_transform(self.X)


        # TSNE perplexity and n-iter are important
        if self.transform in ("tsne", "TSNE", "t-SNE", "T-SNE"):
            if not self.quiet:
                print("training T-SNE model for data tranform")
            tsne = TSNE(
                n_components=2,
                perplexity=50,
                init="pca",
                n_iter=1000,
                random_state=0
            )
            self.X = tsne.fit_transform(self.X)



    def train_test_split(self, prop=0.33):
        "Use sckkit to split dataset"

        # split data and labels
        a, b, c, d = train_test_split(
            self.X, 
            self.y, 
            test_size=prop,
            random_state=self.seed,
            )

        # store as attributes
        self.X_train = a
        self.X_test = b
        self.y_train = c
        self.y_test = d

        # print to screen
        if not self.quiet:
            print("split train/test data: {}/{}".format(
                self.X_train.shape, self.X_test.shape))



    def train_model(self):
        """
        Train ML model...
        """
        if not self.quiet:
            print("training ExtraTrees model...")

        # select the model
        self.model = ExtraTreesClassifier(
            n_estimators=1000, 
            # max_features=np.sqrt(n_features).astype(int),
            n_jobs=4, 
            random_state=self.seed,
        )

        # fit the model to training data set
        self.model.fit(self.X_train, self.y_train)

        # print score to screen
        if not self.quiet:
            print("model score on training set: {:.3f}".format(
                self.model.score(self.X_train, self.y_train)
                ))
            print("model score on test/validation set: {:.3f}".format(
                self.model.score(self.X_test, self.y_test)
                ))



    def _run(self, ipyclient, children=[]):

        # load-balancer for single-threaded execution jobs
        lbview = ipyclient.load_balanced_view()

        # get all edges on the tree
        edges = get_all_admix_edges(self.tree)

        # other params to iterate over
        aprops = np.arange(0.05, 0.30, 0.05)
        nsnps = np.arange(5000, 55000, 5000)
        reps = np.arange(10)

        # total number of samples
        ns = len(edges) * aprops.size * nsnps.size * reps.size

        # build empty data frame
        validate = pd.DataFrame({
            "admix_edge": [0] * ns,
            "admix_prop": 0,
            "nsnps": 0,
            "est": 0,
            "prob": 0,
        }, columns=["nsnps", "admix_edge", "admix_prop", "est", "prob"],
        )

        # dataframe index 
        idx = 0
        rasyncs = {}

        # simulate a 
        for edge in edges:
            for prop in aprops:
                for snp in nsnps:
                    for rep in reps:

                        # submit jobs
                        kwargs = {
                            'tree': self.tree, 
                            'admixture_edges': [(edge[0], edge[1], 0.5, prop)],
                            'nsnps': snp,
                            'run': True,
                        }
                        rasyncs[idx] = lbview.apply(Model, **kwargs)

                        # fill labels
                        validate.loc[idx, :3] = snp, edge, prop


                        # edgestr = str(edge).replace(" ", "")[1:-1]
                        # testX = test.counts.flatten().reshape((1, -1))
                        # est = self.model.predict(testX)
                        # probs = self.model.predict_proba(testX)[0]
                        # tidx = list(self.model.classes_).index(edgestr)
                        # prob = probs[tidx]
                        # validate.loc[idx] = rep, nsnps, edgestr, magn, est, prob
                        idx += 1


    
    def run(self, force=True, ipyclient=None, show_cluster=False, auto=False):
        """
        Build a DataFrame of predictive accuracy scores for a trained model
        when new data is simulated on it under a range of input settings.

        Parameters:
        -----------
        ipyclient (ipyparallel.Client object):
            A connected ipyclient object. If ipcluster instance is 
            not running on the default profile then ...
        """
        # distribute filling jobs in parallel
        pool = Parallel(
            tool=self,
            rkwargs={},
            ipyclient=ipyclient,
            show_cluster=show_cluster,
            auto=auto,
            )
        pool.wrap_run()



    def tsne_plot(self):


        # build color vector
        colors = [0] * self.df.shape[0]
        for idx in self.df.index:
            if self.df.sisters[idx]:
                colors[idx] = 1
            if self.df.asource[idx] == 5:
                colors[idx] = 2
            if self.df.asource[idx] == 4:
                colors[idx] = 3
        colors = [toyplot.color.Palette()[i] for i in colors]


        # save a 3D plot if 3-dimension...
        # TODO w/ ipyvolume.