#!/usr/bin/env python

"""
Generate a large database ...
"""

# imports for py3 compatibility
from __future__ import print_function
from builtins import range

import os
import h5py
import time

import toytree
import numpy as np
import itertools as itt
from scipy.special import comb

from .Model import Model
from .Simulator import Simulator
from .parallel import Parallel
from .utils import get_all_admix_edges, SimcatError, Progress, tile_reps


############################################################################
class Database:
    """
    An object to build a HD5 database with parameters (labels) for simulations.
    The number of labeled tests is equal to nevents * ntests * nreps,
    where nevents is based on the tree topology and number of admixture edges
    drawn on it (nedges). Typical use is to provide a fixed ultrametric tree
    and build the database for all placements of one or two admixture edges
    on the tree.

    Parameters:
    -----------
    name: str
        The name that will be used in the saved database file (<name>.hdf5)

    workdir: str
        The location where the database file will be saved, or loaded from
        if continuing an analysis from a checkpoint.

    tree: newick or toytree
        A fixed topology to use for all simulations. Edge lengths are fixed
        unless the argument 'edge_function' is used, in which case edge lengths
        are drawn from a distribution.

    nedges: int (default=0)
        The number of admixture edges to add to each tree at a time. All edges
        will be drawn on the tree that can connect any branches which overlap
        for a nonzero amount of time. A set of admixture scenarios
        generated by drawing nedges on a tree is referred to as nevents, and
        all possible events will be tested.
        * Each nedge increases nvalues by nevents * ntests * nreps.

    ntests: int (default=100)
        The number of parameters to draw for each admixture_event described
        by an edge but sampling different durations, magnitudes, and mutation
        rates (theta). For example, (2, 1, None, None, None) could draw
        (2, 1, 0.1, 0.3, 0.01) and theta=0.1 in one randomly drawn test,
        and (2, 1, 0.2, 0.4, 0.02) and theta=0.2 in another.
        * Each ntest increases nvalues by nreps.

    nreps: int (default=10)
        The number of replicate simulations to run per admixture scenario,
        sampled tree, and parameter set (nevent, ntree, ntest). Replicate
        simulations make identical calls to msprime but get variable result
        matrices due to variability in the coalescent process.

    nsnps: int (default=1000)
        The number of SNPs in each simulation that are used to build the
        16x16 arrays of phylogenetic invariants for each quartet sample.

    theta: int or tuple (default=0.01)
        The mutation parameter (2*Ne*u), or range of values from which values
        will be uniformly drawn across ntests.

    seed: int (default=123)
        Set the seed of the random number generator

    force: bool (default=False)
        Force overwrite of existing database file.
    """
    def __init__(
        self,
        name,
        workdir,
        tree,
        nsnps=10000,
        nedges=1,
        ntests=1,
        nreps=1,
        Ne=10000,
        seed=123,
        admix_prop_min=0.05,
        admix_prop_max=0.50,
        admix_edge_min=0.5,
        admix_edge_max=0.5,
        exclude_sisters=False,
        node_slider=True,
        force=False,
        quiet=False,
        ):

        # database locations
        self.name = name
        self.workdir = (
            workdir if workdir
            else os.path.realpath(os.path.join('.', "databases")))
        # labels data file
        self.labels = os.path.realpath(
            os.path.join(workdir, "{}.labels.h5".format(self.name)))
        self.counts = os.path.realpath(
            os.path.join(workdir, "{}.counts.h5".format(self.name)))
        self.checkpoint = 0
        self._quiet = quiet
        self.node_slider = node_slider

        # store params
        self.Ne = Ne
        self.tree = (
            toytree.tree(tree) if isinstance(tree, str) else tree.copy())
        self.admix_edge_min = admix_edge_min
        self.admix_edge_max = admix_edge_max
        self.admix_prop_min = admix_prop_min
        self.admix_prop_max = admix_prop_max
        self.exclude_sisters = exclude_sisters

        # database label combinations
        self.nedges = nedges
        self.ntests = ntests
        self.nreps = nreps
        self.nsnps = nsnps
        admixedges = get_all_admix_edges(self.tree, 0.0, 1.0, exclude_sisters)
        nevents = int(comb(N=len(admixedges), k=self.nedges))
        nvalues = nevents * self.ntests * self.nreps
        self.nstored_values = nvalues

        self.init_databases(force)

        # progress
        if not self._quiet:
            shortpath = self.labels
            if os.path.abspath("..") in shortpath:
                shortpath = shortpath.replace(os.path.abspath(".."), "..")
            if os.path.expanduser("~") in shortpath:
                shortpath = shortpath.replace(os.path.expanduser("~"), "~")
            print("{} labels stored in: {}".format(
                self.nstored_values, shortpath)
            )       

        # decide on an appropriate chunksize to keep memory load reasonable
        self.chunksize = 100

        # store ipcluster information
        self.ipcluster = {
            "cluster_id": "",
            "profile": "default",
            "engines": "Local",
            "quiet": 0,
            "timeout": 60,
            "cores": 0,
            "threads": 2,
            "pids": {},
        }

        # make sure workdir exists
        if not os.path.exists(workdir):
            os.makedirs(workdir)

    def database_status(self):
        """
        Prints to screen info about the size of the database files and the
        progress/checkpoint for filling the databases.
        """
        print(self.nstored_values)
        with h5py.File(self.labels) as io5:
            keys = io5.keys()
            for key in keys:
                print(key, io5[key].shape)

        with h5py.File(self.counts) as io5:
            print('counts', io5["counts"].shape)

    def init_databases(self, force=False):
        """
        Parses parameters in self.params to create all combinations
        of parameter values to test. Returns the number of the simulations.
        Simulation metadata is appended to datasets.

        Expect that the h5 file self._db is open in w or a mode.
        """
        # create database in 'w-' mode to prevent overwriting
        if not os.path.exists(self.labels):
            self.i5 = h5py.File(self.labels, mode='w')
            self.o5 = h5py.File(self.counts, mode='w')
        else:
            if force:
                self.i5 = h5py.File(self.labels, mode='w')
                self.o5 = h5py.File(self.counts, mode='w')
            else:
                self.i5 = h5py.File(self.labels, mode='a')
                self.o5 = h5py.File(self.counts, mode='a')

        # store the tree
        itree = self.tree.copy()
        #for node in itree.treenode.traverse():
        #    node.name = node.idx
        self.i5.attrs["tree"] = itree.write()
        self.o5.attrs["tree"] = itree.write()
        self.i5.attrs["nsnps"] = self.nsnps
        self.o5.attrs["nsnps"] = self.nsnps

        # number of quartets depends only on size of tree
        nquarts = int(comb(N=len(itree), k=4))

        # store count matrices (the data)
        self.o5.create_dataset("counts",
                               shape=(self.nstored_values, nquarts*16*16),
                               dtype=np.int64)

        # array of node heights, columns corresponding to node idx
        self.i5.create_dataset("node_heights",
                               shape=(self.nstored_values, itree.nnodes),
                               dtype=np.float64)
        # array of node Nes, columns corresponding to node idx
        self.i5.create_dataset("node_Nes",
                               shape=(self.nstored_values, itree.nnodes),
                               dtype=np.int64)
        # is there an admixture event on this row?
        self.i5.create_dataset("admixed",
                               shape=(self.nstored_values,),
                               dtype=np.bool_)
        # what are the admixture paramters? (directly inputable to simulation)
        self.i5.create_dataset("admixture_args",
                               shape=(self.nstored_values,),
                               dtype=h5py.string_dtype(encoding='ascii'))

        # make a list of the unique trees
        # if we ever want to slide nodes around, adjust node Nes, etc...
        # We can probably do it here on this list of trees
        idx = 0
        # for now just a constant Ne...
        Nes = np.repeat(self.Ne, itree.nnodes)
        for node in itree.treenode.traverse():
            node.add_feature('Ne', Nes[idx])
            idx += 1
        if not self.node_slider:
            all_trees = np.repeat(itree,
                                  self.ntests)

        else:
            all_trees = np.array([itree.mod.node_slider() for i in range(self.ntests)])

        eidx = 0
        for unique_tree in all_trees:  # note that this could also be a tree generator...

            # get_all_admix_edges for each tree... because different trees
            # will have different edges
            admixedges = get_all_admix_edges(unique_tree,
                                             0.0,
                                             1.0,
                                             self.exclude_sisters)
            events = itt.combinations(admixedges.keys(), self.nedges)
            # tuple of admixture_edges as tuples with ranges to sample.
            for evt in events:
                for rep in range(self.nreps):
                    for node in unique_tree.treenode.traverse():
                        self.i5['node_heights'][eidx, node.idx] = node.height
                        self.i5['node_Nes'][eidx, node.idx] = node.Ne
                    admixlist = [
                        (
                            i[0],
                            i[1],
                            np.random.uniform(self.admix_edge_min,
                                              self.admix_edge_max),
                            np.random.uniform(self.admix_prop_min,
                                              self.admix_prop_max),
                        )
                        for i in evt
                    ]
                    admix_arg = str(admixlist).encode('ascii')
                    # in case we want to include an option for no admixture
                    self.i5['admixed'][eidx] = True
                    self.i5['admixture_args'][eidx] = admix_arg
                    eidx += 1



        # close the files
        self.i5.close()
        self.o5.close()

    def _run(self, ipyclient, children=[]):
        """
        Sends jobs to parallel engines to run Simulator.run().
        """
        # if outfile exists and not force then find checkpoint
        # ...

        # load-balancer for single-threaded execution jobs
        lbview = ipyclient.load_balanced_view()

        # set chunksize based on ncores and nstored_values
        ncores = len(ipyclient)
        self.chunksize = int(np.ceil(self.nstored_values / (ncores * 4)))
        self.chunksize = min(500, self.chunksize)
        self.chunksize = max(4, self.chunksize)

        # an iterator to return chunked slices of jobs
        jobs = range(self.checkpoint, self.nstored_values, self.chunksize)
        njobs = int((self.nstored_values - self.checkpoint) / self.chunksize)

        # submit jobs to engines
        rasyncs = {}
        for slice0 in jobs:
            slice1 = min(self.nstored_values, slice0 + self.chunksize)
            if slice1 > slice0:
                args = (self.labels, slice0, slice1)
                rasyncs[slice0] = lbview.apply(Simulator, *args)

        # catch results as they return and enter into H5 to keep mem low.
        progress = Progress(njobs, "Simulating count matrices", children)
        progress.increment_all(self.checkpoint)
        if not self._quiet:
            progress.display()
        done = self.checkpoint
        try:
            io5 = h5py.File(self.counts, mode='r+')
            while 1:
                ## gather finished jobs
                finished = [i for i, j in rasyncs.items() if j.ready()]

                ## iterate over finished list and insert results
                for job in finished:
                    rasync = rasyncs[job]
                    if rasync.successful():

                        # store result
                        done += 1
                        progress.increment_all()
                        result = rasync.get().counts                       
                        io5["counts"][job:job + self.chunksize] = result

                        # free up memory from job
                        del rasyncs[job]

                    else:
                        raise SimcatError(rasync.get())

                # print progress
                progress.increment_time()

                # clear progress bar occasionally to avoid iopub rates?
                # ...would this work?

                # finished: break loop
                if len(rasyncs) == 0:
                    break
                else:
                    time.sleep(0.5)

            # on success: close the progress counter
            progress.widget.close()
            # print finished message
            # if not self._quiet:
            #     shortpath = self.counts
            #     if os.path.abspath("..") in shortpath:
            #         shortpath = shortpath.replace(os.path.abspath(".."), "..")
            #     if os.path.expanduser("~") in shortpath:
            #         shortpath = shortpath.replace(os.path.expanduser("~"), "~")
            #     print("{} matrices stored in: {}".format(
            #         self.nstored_values,
            #         shortpath)
            #     )

        finally:
            # close the hdf5 handle
            io5.close()

    def run(self, force=True, ipyclient=None, show_cluster=False, auto=False):
        """
        Distributes parallel jobs and wraps functions for convenient cleanup.
        Fills all count matrices with simulated data.

        Parameters
        ----------
        ipyclient (ipyparallel.Client object):
            A connected ipyclient object. If ipcluster instance is
            not running on the default profile then ...
        """
        # Fill all params into the database (this inits the Model objects
        # which call ._get_test_values() to generate all simulation scenarios.
        self.init_databases(force=force)

        # distribute filling jobs in parallel
        pool = Parallel(
            tool=self,
            rkwargs={},
            ipyclient=ipyclient,
            show_cluster=show_cluster,
            auto=auto,
            quiet=self._quiet,
            )
        pool.wrap_run()
