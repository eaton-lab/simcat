#!/usr/bin/env python

"""
Generate a large database ...
"""

# imports for py3 compatibility
from __future__ import print_function
from builtins import range

import os
import h5py
import time

import toytree
import numpy as np
import itertools as itt
from scipy.special import comb

from .Model import Model
from .Simulator import Simulator
from .parallel import Parallel
from .utils import get_all_admix_edges, SimcatError, Progress, tile_reps


############################################################################
class Database:
    """
    An object to build a HD5 database with parameters (labels) for simulations.
    The number of labeled tests is equal to nevents * ntests * nreps, 
    where nevents is based on the tree topology and number of admixture edges 
    drawn on it (nedges). Typical use is to provide a fixed ultrametric tree
    and build the database for all placements of one or two admixture edges 
    on the tree. 

    Parameters:
    -----------
    name: str
        The name that will be used in the saved database file (<name>.hdf5)

    workdir: str
        The location where the database file will be saved, or loaded from 
        if continuing an analysis from a checkpoint. 

    tree: newick or toytree
        A fixed topology to use for all simulations. Edge lengths are fixed
        unless the argument 'edge_function' is used, in which case edge lengths 
        are drawn from a distribution.

    nedges: int (default=0)
        The number of admixture edges to add to each tree at a time. All edges
        will be drawn on the tree that can connect any branches which overlap 
        for a nonzero amount of time. A set of admixture scenarios 
        generated by drawing nedges on a tree is referred to as nevents, and 
        all possible events will be tested. 
        * Each nedge increases nvalues by nevents * ntests * nreps. 

    ntests: int (default=100)
        The number of parameters to draw for each admixture_event described
        by an edge but sampling different durations, magnitudes, and mutation
        rates (theta). For example, (2, 1, None, None, None) could draw 
        (2, 1, 0.1, 0.3, 0.01) and theta=0.1 in one randomly drawn test, 
        and (2, 1, 0.2, 0.4, 0.02) and theta=0.2 in another. 
        * Each ntest increases nvalues by nreps. 

    nreps: int (default=10)
        The number of replicate simulations to run per admixture scenario, 
        sampled tree, and parameter set (nevent, ntree, ntest). Replicate 
        simulations make identical calls to msprime but get variable result
        matrices due to variability in the coalescent process.

    nsnps: int (default=1000)
        The number of SNPs in each simulation that are used to build the 
        16x16 arrays of phylogenetic invariants for each quartet sample. 

    theta: int or tuple (default=0.01)
        The mutation parameter (2*Ne*u), or range of values from which values
        will be uniformly drawn across ntests. 

    seed: int (default=123)
        Set the seed of the random number generator

    force: bool (default=False)
        Force overwrite of existing database file.
    """
    def __init__(
        self,
        name,
        workdir,
        tree,
        nsnps=10000,
        nedges=0,
        ntests=1,
        nreps=1,
        theta=0.01,
        seed=123,
        admix_prop_min=0.05,
        admix_prop_max=0.50,
        admix_edge_min=0.5, 
        admix_edge_max=0.5,
        exclude_sisters=False,
        mutator='msprime',
        force=False,
        quiet=False,
        ):

        # database locations
        self.name = name
        self.workdir = (
            workdir if workdir 
            else os.path.realpath(os.path.join('.', "databases")))
        # labels data file
        self.labels = os.path.realpath(
            os.path.join(workdir, "{}.labels.h5".format(self.name)))
        self.counts = os.path.realpath(
            os.path.join(workdir, "{}.counts.h5".format(self.name)))
        self.checkpoint = 0
        self.mutator = mutator
        self._quiet = quiet

        # store params
        self.theta = theta
        self.tree = (
            toytree.tree(tree) if isinstance(tree, str) else tree.copy())
        self.admix_edge_min = admix_edge_min
        self.admix_edge_max = admix_edge_max
        self.admix_prop_min = admix_prop_min
        self.admix_prop_max = admix_prop_max
        self.exclude_sisters = exclude_sisters

        # database label combinations
        self.nedges = nedges
        self.ntests = ntests        
        self.nreps = nreps
        self.nsnps = nsnps
        admixedges = get_all_admix_edges(self.tree, 0.0, 1.0, exclude_sisters)
        nevents = int(comb(N=len(admixedges), k=self.nedges))
        nvalues = nevents * self.ntests * self.nreps 
        self.nstored_values = nvalues

        # progress
        if not self._quiet:
            shortpath = self.labels
            if os.path.abspath("..") in shortpath:
                shortpath = shortpath.replace(os.path.abspath(".."), "..")
            if os.path.expanduser("~") in shortpath:
                shortpath = shortpath.replace(os.path.expanduser("~"), "~")
            print("{} labels stored in: {}".format(
                self.nstored_values, shortpath)
            )       

        # decide on an appropriate chunksize to keep memory load reasonable
        self.chunksize = 100

        # store ipcluster information 
        self.ipcluster = {
            "cluster_id": "", 
            "profile": "default",
            "engines": "Local", 
            "quiet": 0, 
            "timeout": 60, 
            "cores": 0, 
            "threads": 2,
            "pids": {},
        }

        # make sure workdir exists
        if not os.path.exists(workdir):
            os.makedirs(workdir)


    def database_status(self):
        """
        Prints to screen info about the size of the database files and the 
        progress/checkpoint for filling the databases. 
        """
        print(self.nstored_values)
        with h5py.File(self.labels) as io5:
            keys = io5.keys()
            for key in keys:
                print(key, io5[key].shape)
        
        with h5py.File(self.counts) as io5:
            print('counts', io5["counts"].shape)


    def init_databases(self, force=False):
        """
        Parses parameters in self.params to create all combinations
        of parameter values to test. Returns the number of the simulations.
        Simulation metadata is appended to datasets. 

        Expect that the h5 file self._db is open in w or a mode.
        """       
        # create database in 'w-' mode to prevent overwriting
        if not os.path.exists(self.labels):
            self.i5 = h5py.File(self.labels, mode='w')
            self.o5 = h5py.File(self.counts, mode='w')            
        else:
            if force:
                self.i5 = h5py.File(self.labels, mode='w')
                self.o5 = h5py.File(self.counts, mode='w')                
            else:
                self.i5 = h5py.File(self.labels, mode='a')
                self.o5 = h5py.File(self.counts, mode='a')                

        # store the tree as newick using idx for name, 
        itree = self.tree.copy()
        for node in itree.treenode.traverse():
            node.name = node.idx
        self.i5.attrs["tree"] = itree.write()
        self.o5.attrs["tree"] = itree.write()        
        self.i5.attrs["nsnps"] = self.nsnps
        self.o5.attrs["nsnps"] = self.nsnps

        # number of quartets depends only on size of tree
        nquarts = int(comb(N=len(itree), k=4))

        # store count matrices (the data)
        self.o5.create_dataset("counts", 
            shape=(self.nstored_values, nquarts*16*16),
            dtype=np.int64)

        # the labels: for pulsed migration admix_tstarts but not admix_tends
        self.i5.create_dataset("admixed", 
            shape=(self.nstored_values,),
            dtype=np.bool_)
        self.i5.create_dataset("admix_sources", 
            shape=(self.nstored_values, self.nedges),
            dtype=np.uint8)
        self.i5.create_dataset("admix_targets", 
            shape=(self.nstored_values, self.nedges),
            dtype=np.uint8)
        self.i5.create_dataset("admix_props", 
            shape=(self.nstored_values, self.nedges),
            dtype=np.float64)
        self.i5.create_dataset("admix_times", 
            shape=(self.nstored_values, self.nedges),
            dtype=np.float64)
        self.i5.create_dataset("thetas",
            shape=(self.nstored_values,),
            dtype=np.float64)

        # get all admixture edges that can be drawn on this tree
        admixedges = get_all_admix_edges(itree, 0.0, 1.0, self.exclude_sisters)

        # (2) nevents: iterate over each edge or pair of edges (nedges)        
        eidx = 0
        events = itt.combinations(admixedges.keys(), self.nedges)
        for evt in events:

            # tuple of admixture_edges as tuples with ranges to sample.
            admixlist = [
                (
                    i[0], 
                    i[1], 
                    (self.admix_edge_min, self.admix_edge_max), 
                    (self.admix_prop_min, self.admix_prop_max),
                )
                for i in evt
            ]

            # (3) ntests: sample duration, magnitude, and params on edges
            model = Model(
                tree=itree, 
                ntests=self.ntests,
                theta=self.theta,
                admixture_edges=admixlist,
            )

            # (4) nreps: fill the same param values repeated times
            mdict = model.test_values
            nnn = self.ntests * self.nreps
            sta, end = eidx, eidx + nnn

            # store thetas same for every rep, but not test (0,0,0,1,1,1)
            thetas = tile_reps(mdict["thetas"], self.nreps)
            self.i5["thetas"][sta:end] = thetas

            # get labels from admixlist and model.test_values
            for xidx in range(model.aedges):
                sources = np.repeat(admixlist[xidx][0], nnn)
                targets = np.repeat(admixlist[xidx][1], nnn)
                mrates = tile_reps(mdict[xidx]["mrates"], self.nreps)
                mtimes = tile_reps(mdict[xidx]["mtimes"][:, 0], self.nreps)
                # mend = tile_reps(mdict[xidx]["mtimes"][:, 1], self.nreps)                    

                # store labels for this admix event (nevents x nreps)
                self.i5["admix_sources"][sta:end, xidx] = sources
                self.i5["admix_targets"][sta:end, xidx] = targets
                self.i5["admix_props"][sta:end, xidx] = mrates
                self.i5["admix_times"][sta:end, xidx] = mtimes.astype(np.float64)
            eidx += nnn

        # close shop
        self.i5.close()
        self.o5.close()
              

    def _run(self, ipyclient, children=[]):
        """
        Sends jobs to parallel engines to run Simulator.run().
        """
        # if outfile exists and not force then find checkpoint
        # ...

        # load-balancer for single-threaded execution jobs
        lbview = ipyclient.load_balanced_view()

        # set chunksize based on ncores and nstored_values
        ncores = len(ipyclient)
        self.chunksize = int(np.ceil(self.nstored_values / (ncores * 4)))
        self.chunksize = min(500, self.chunksize)
        self.chunksize = max(4, self.chunksize)        

        # an iterator to return chunked slices of jobs
        jobs = range(self.checkpoint, self.nstored_values, self.chunksize)
        njobs = int((self.nstored_values - self.checkpoint) / self.chunksize)

        # submit jobs to engines
        rasyncs = {}
        for slice0 in jobs:
            slice1 = min(self.nstored_values, slice0 + self.chunksize)
            if slice1 > slice0:
                args = (self.labels, slice0, slice1,self.mutator)
                rasyncs[slice0] = lbview.apply(Simulator, *args)

        # catch results as they return and enter into H5 to keep mem low.
        progress = Progress(njobs, "Simulating count matrices", children)
        progress.increment_all(self.checkpoint)
        if not self._quiet:
            progress.display()
        done = self.checkpoint
        try:
            io5 = h5py.File(self.counts, mode='r+')
            while 1:
                ## gather finished jobs
                finished = [i for i, j in rasyncs.items() if j.ready()]

                ## iterate over finished list and insert results
                for job in finished:
                    rasync = rasyncs[job]
                    if rasync.successful():

                        # store result
                        done += 1
                        progress.increment_all()
                        result = rasync.get().counts                       
                        io5["counts"][job:job + self.chunksize] = result

                        # free up memory from job
                        del rasyncs[job]

                    else:
                        raise SimcatError(rasync.get())

                # print progress
                progress.increment_time()

                # clear progress bar occasionally to avoid iopub rates?
                # ...would this work?

                # finished: break loop
                if len(rasyncs) == 0:
                    break
                else:
                    time.sleep(0.5)

            # on success: close the progress counter
            progress.widget.close()
            # print finished message
            # if not self._quiet:
            #     shortpath = self.counts
            #     if os.path.abspath("..") in shortpath:
            #         shortpath = shortpath.replace(os.path.abspath(".."), "..")
            #     if os.path.expanduser("~") in shortpath:
            #         shortpath = shortpath.replace(os.path.expanduser("~"), "~")
            #     print("{} matrices stored in: {}".format(
            #         self.nstored_values, 
            #         shortpath)
            #     )

        finally:
            # close the hdf5 handle
            io5.close()



    def run(self, force=True, ipyclient=None, show_cluster=False, auto=False):
        """
        Distributes parallel jobs and wraps functions for convenient cleanup.
        Fills all count matrices with simulated data. 

        Parameters
        ----------
        ipyclient (ipyparallel.Client object):
            A connected ipyclient object. If ipcluster instance is 
            not running on the default profile then ...
        """
        # Fill all params into the database (this inits the Model objects 
        # which call ._get_test_values() to generate all simulation scenarios.
        self.init_databases(force=force)

        # distribute filling jobs in parallel
        pool = Parallel(
            tool=self,
            rkwargs={},
            ipyclient=ipyclient,
            show_cluster=show_cluster,
            auto=auto,
            quiet=self._quiet,
            )
        pool.wrap_run()

