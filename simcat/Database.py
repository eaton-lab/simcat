#!/usr/bin/env python

"""
Generate a large database ...
"""

# imports for py3 compatibility
from __future__ import print_function
from builtins import range

import os
import h5py
import time
import itertools as itt

import toytree
import numpy as np

from .parallel import Parallel
from .Simulator import IPCoalWrapper
from .utils import get_all_admix_edges, SimcatError, Progress



class Database:
    """
    An object to build a HD5 database with parameters (labels) for simulations.
    The number of labeled tests is equal to nevents * ntests * nreps,
    where nevents is based on the tree topology and number of admixture edges
    drawn on it (nedges). Typical use is to provide a fixed ultrametric tree
    and build the database for all placements of one or two admixture edges
    on the tree.

    Parameters:
    -----------
    name: str
        The name that will be used in the saved database file (<name>.hdf5)

    workdir: str
        The location where the database file will be saved, or loaded from
        if continuing an analysis from a checkpoint.

    tree: newick or toytree
        A fixed topology to use for all simulations. Edge lengths are fixed
        unless the argument 'edge_function' is used, in which case edge lengths
        are drawn from a distribution.

    nedges: int (default=0)
        The number of admixture edges to add to each tree at a time. All edges
        will be drawn on the tree that can connect any branches which overlap
        for a nonzero amount of time. A set of admixture scenarios
        generated by drawing nedges on a tree is referred to as nevents, and
        all possible events will be tested.
        * Each nedge increases nvalues by nevents * ntests * nreps.

    ntests: int (default=100)
        The number of parameters to draw for each admixture_event described
        by an edge but sampling different durations, magnitudes, and mutation
        rates (theta). For example, (2, 1, None, None, None) could draw
        (2, 1, 0.1, 0.3, 0.01) and theta=0.1 in one randomly drawn test,
        and (2, 1, 0.2, 0.4, 0.02) and theta=0.2 in another.
        * Each ntest increases nvalues by nreps.

    nreps: int (default=10)
        The number of replicate simulations to run per admixture scenario,
        sampled tree, and parameter set (nevent, ntree, ntest). Replicate
        simulations make identical calls to msprime but get variable result
        matrices due to variability in the coalescent process.

    nsnps: int (default=1000)
        The number of SNPs in each simulation that are used to build the
        16x16 arrays of phylogenetic invariants for each quartet sample.

    seed: int (default=None)
        Set the seed of the random number generator

    force: bool (default=False)
        Force overwrite of existing database file.
    """
    def __init__(
        self,
        name,
        workdir,
        tree,
        nsnps=10000,
        nedges=1,
        ntests=1,
        nreps=1,
        Ne=10000,
        admix_prop_min=0.05,
        admix_prop_max=0.50,
        admix_edge_min=0.5,
        admix_edge_max=0.5,
        exclude_sisters=False,
        node_slider=True,
        seed=None,
        force=False,
        quiet=False,
        random_sampling=True,
        nthreads=2,
        ):

        # init random seed generator
        self.random = np.random.RandomState(seed)

        # database locations
        self.name = name
        self.workdir = (
            workdir if workdir else os.path.realpath("./databases"))
        if not os.path.exists(workdir):
            os.makedirs(workdir)

        # labels data file
        self.labels = os.path.realpath(
            os.path.join(workdir, "{}.labels.h5".format(self.name)))
        self.counts = os.path.realpath(
            os.path.join(workdir, "{}.counts.h5".format(self.name)))
        self.checkpoint = 0
        self._quiet = quiet
        self._random_sampling = random_sampling
        self._nthreads = nthreads

        # store params
        self.Ne = Ne
        self.tree = (
            toytree.tree(tree) if isinstance(tree, str) else tree.copy())
        self._get_Ne()
        self.inodes = self.tree.nnodes - self.tree.ntips
        self.node_slider = node_slider

        self.admix_edge_min = admix_edge_min
        self.admix_edge_max = admix_edge_max
        self.admix_prop_min = admix_prop_min
        self.admix_prop_max = admix_prop_max
        self.exclude_sisters = exclude_sisters

        # database label combinations
        self.nedges = nedges
        self.ntests = ntests
        self.nreps = nreps
        self.nsnps = nsnps
        self.nquarts = sum(1 for i in itt.combinations(range(tree.ntips), 4))

        # get number of places to put admix edges on THIS tree. If node slider
        # is on then we might observe other edges, in which case there will 
        # just be fewer of these edges in that 'test'. In each test the order
        # of placement of admix edges will be random so that node slide admix
        # edges that get added can be worked in without changing the total 
        # number of tests. 
        args = (self.tree, 0.5, 0.5, self.exclude_sisters)
        admixedges = get_all_admix_edges(*args)
        self.aedges = list(itt.combinations(admixedges, self.nedges))
        self.naedges = len(self.aedges)
        self.nstored_labels = self.naedges * self.ntests * self.nreps

        # create or clear the database for writing
        self.init_databases(force)

        # print to user a progress report 
        if not self._quiet:
            shortpath = self.labels
            if os.path.abspath("..") in shortpath:
                shortpath = shortpath.replace(os.path.abspath(".."), "..")
            if os.path.expanduser("~") in shortpath:
                shortpath = shortpath.replace(os.path.expanduser("~"), "~")
            print("{} labels to be stored in: {}".format(
                self.nstored_labels, shortpath)
            )

        # decide on an appropriate chunksize to keep memory load reasonable
        self.chunksize = 100

        # store ipcluster information
        self.ipcluster = {
            "cluster_id": "",
            "profile": "default",
            "engines": "Local",
            "quiet": 0,
            "timeout": 60,
            "cores": 0,
            "threads": 2,
            "pids": {},
        }

        # fill the database
        self.fill_labels_database()



    def database_status(self):
        """
        Prints to screen info about the size of the database files and the
        progress/checkpoint for filling the databases.
        """
        print(self.nstored_labels)
        with h5py.File(self.labels) as io5:
            for key in io5.keys:
                print(key, io5[key].shape)

        with h5py.File(self.counts) as io5:
            print('counts', io5["counts"].shape)



    def _get_Ne(self):
        """
        If Ne node attrs are present in the input tree these override the 
        global Ne argument which is set to all other nodes. Every node should
        have an Ne value at the end of this. Sets node.Ne attrs and sets max
        value to self.Ne.
        """
        # get map of {nidx: node}
        ndict = self.tree.get_node_dict(True, True)

        # set user entered arg (self.Ne) to any node without a Ne attr
        for nidx, node in ndict.items():
            if not hasattr(node, "Ne"):
                setattr(node, "Ne", int(self.Ne))
            else:
                setattr(node, "Ne", int(node.Ne))

        # check that all nodes have .Ne
        nes = self.tree.get_node_values("Ne", True, True)
        if not all(nes):
            raise SimcatError(
                "Ne must be provided as an argument or tree node attribute.")

        # set to the max value        
        self.Ne = max(nes)



    def init_databases(self, force=False):
        """
        Parses parameters in self.params to create all combinations
        of parameter values to test. Returns the number of the simulations.
        Simulation metadata is appended to datasets.

        Expect that the h5 file self._db is open in w or a mode.
        """
        # create database in 'w-' mode to prevent overwriting
        if not os.path.exists(self.labels):
            i5 = h5py.File(self.labels, mode='w')
            o5 = h5py.File(self.counts, mode='w')
        else:
            if force:
                i5 = h5py.File(self.labels, mode='w')
                o5 = h5py.File(self.counts, mode='w')
            else:
                return 
                # i5 = h5py.File(self.labels, mode='a')
                # o5 = h5py.File(self.counts, mode='a')

        # store some database attribute info
        i5.attrs["tree"] = self.tree.write()
        o5.attrs["tree"] = self.tree.write()
        i5.attrs["nsnps"] = self.nsnps
        o5.attrs["nsnps"] = self.nsnps
        i5.attrs["nquarts"] = self.nquarts
        o5.attrs["nquarts"] = self.nquarts

        # store data in separate dsets and with matrix shape so that in the 
        # analysis we can best take advantage of different combinations of the
        # data and its structure.
        smat = (self.nstored_labels, self.nquarts, 16, 16)

        # countsize = (self.nstored_labels, snps + svdu + svdv + svds + mvar)
        o5.create_dataset(name="counts", shape=smat, dtype=np.int64, compression="gzip")

        # array of node heights,Nes in traverse order (-tips)
        lnodes = (self.nstored_labels, self.inodes)
        i5.create_dataset(name="node_heights", shape=lnodes, dtype=np.int64)
        i5.create_dataset(name="node_Nes", shape=lnodes, dtype=np.int64)
        i5.create_dataset(name="slide_seeds", shape=(lnodes[0],), dtype=np.int)

        # array of admixture triplets (source, dest, prop) time is always p0.5
        ashape = (self.nstored_labels, 3 * self.nedges)
        i5.create_dataset(name="admixture", shape=ashape, dtype=np.float64)

        # close the files
        i5.close()
        o5.close()



    def fill_labels_database(self):
        """
        Fill the h5 database with all labels.
        """
        # sample admixture props randomly or uniformly?
        if not self._random_sampling:
            migsamps = np.linspace(
                self.admix_prop_min, self.admix_prop_max, self.ntests)
        else:
            migsamps = self.random.uniform(
                self.admix_prop_min, self.admix_prop_max, self.ntests)

        # arrays to write in chunks to the h5 array
        chunksize = 10000
        arr_h = np.zeros((chunksize, self.inodes), dtype=np.int)
        arr_n = np.zeros((chunksize, self.inodes), dtype=np.int)
        arr_a = np.zeros((chunksize, 3), dtype=np.float)
        arr_s = np.zeros((chunksize,), dtype=np.int)

        # test is a sampled nodeslide (heights, edges), migrate, migprop, Nes
        wdx = 0
        idx = 0
        for test in range(self.ntests):

            # wiggle node heights
            if self.node_slider:
                slide_seed = self.random.randint(0, 1e12)
                ntree = self.tree.mod.node_slider(prop=0.25, seed=slide_seed)
            else:
                ntree = self.tree

            # store internal heights and Nes to array
            heights = ntree.get_node_values("height", 1, 1).astype(int)
            popsize = ntree.get_node_values("Ne", 1, 1).astype(int)
            mask = heights > 0
            heights = heights[mask]
            popsize = popsize[mask]

            # get n admixture edges (on this slide tree)
            aedges = get_all_admix_edges(ntree, 0.5, 0.5, self.exclude_sisters)

            # if node sliding changed naedges sub or super select random aedges 
            if len(aedges) != self.naedges:
                rgs = range(len(aedges))
                aes = np.random.choice(rgs, self.naedges)
                aedges = np.array(list(aedges))[aes]

            # sample admix proportion
            migprop = migsamps[test]

            # iterate over each placement of the edges
            for edgetup in aedges:

                # simulation replicates
                for rep in range(self.nreps):
                    arr_h[idx] = heights
                    arr_n[idx] = popsize
                    arr_a[idx] = (edgetup[0], edgetup[1], migprop)
                    arr_s[idx] = slide_seed

                    # advance counter
                    idx += 1

                    # reset arrs if bigger than chunksize
                    if (idx == chunksize) or (idx == self.nstored_labels):
                        with h5py.File(self.labels, 'a') as i5:
                            i5["node_heights"][wdx:wdx + idx] = arr_h[:idx]
                            i5["node_Nes"][wdx:wdx + idx] = arr_n[:idx]
                            i5["admixture"][wdx:wdx + idx] = arr_a[:idx]
                            i5["slide_seeds"][wdx:wdx + idx] = arr_s[:idx]
                            arr_h[:] = 0
                            arr_n[:] = 0
                            arr_a[:] = 0
                            wdx += idx
                            idx = 0



    def _run(self, ipyclient, children=[]):
        """
        Sends jobs to parallel engines to run Simulator.run().
        """
        # if outfile exists and not force then find checkpoint
        # ...

        # load-balancer for distributed parallel jobs
        lbview = ipyclient.load_balanced_view()

        # set chunksize based on ncores and stored_labels
        # ncores = len(ipyclient)
        # self.chunksize = int(np.ceil(self.nstored_labels / (ncores * 8)))
        # self.chunksize = min(12, self.chunksize)
        # self.chunksize = max(4, self.chunksize)
        self.chunksize = 4

        # an iterator to return chunked slices of jobs
        jobs = range(self.checkpoint, self.nstored_labels, self.chunksize)
        njobs = int((self.nstored_labels - self.checkpoint) / self.chunksize)

        # submit jobs to engines
        rasyncs = {}
        for slice0 in jobs:
            slice1 = min(self.nstored_labels, slice0 + self.chunksize)
            if slice1 > slice0:
                args = (self.labels, slice0, slice1, self._nthreads, True)
                rasyncs[slice0] = lbview.apply(IPCoalWrapper, *args)

        # catch results as they return and enter into H5 to keep mem low.
        progress = Progress(njobs, "Simulating count matrices", children)
        progress.increment_all(self.checkpoint)
        if not self._quiet:
            progress.display()
        done = self.checkpoint
        try:
            io5 = h5py.File(self.counts, mode='r+')
            while 1:
                # gather finished jobs
                finished = [i for i, j in rasyncs.items() if j.ready()]

                # iterate over finished list and insert results
                for job in finished:
                    rasync = rasyncs[job]
                    if rasync.successful():

                        # store result
                        done += 1
                        progress.increment_all()

                        # object returns, pull out results
                        res = rasync.get()
                        io5["counts"][job:job + self.chunksize, :] = res.counts

                        # free up memory from job
                        del rasyncs[job]

                    else:
                        raise SimcatError(rasync.get())

                # print progress
                progress.increment_time()

                # finished: break loop
                if len(rasyncs) == 0:
                    break
                else:
                    time.sleep(0.5)

            # on success: close the progress counter
            progress.widget.close()
            print(
                "completed {} simulations in {}."
                .format(self.nstored_labels, progress.elapsed)
            )

        finally:
            # close the hdf5 handle
            io5.close()



    def run(self, force=True, ipyclient=None, show_cluster=False, auto=False):
        """
        Distributes parallel jobs and wraps functions for convenient cleanup.
        Fills all count matrices with simulated data.

        Parameters
        ----------
        ipyclient (ipyparallel.Client object):
            A connected ipyclient object. If ipcluster instance is
            not running on the default profile then ...
        """
        # distribute filling jobs in parallel
        pool = Parallel(
            tool=self,
            rkwargs={},
            ipyclient=ipyclient,
            show_cluster=show_cluster,
            auto=auto,
            quiet=self._quiet,
            )
        pool.wrap_run()
