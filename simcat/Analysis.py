#!/usr/bin/env python

"""
Machine learning code for analyzing database info...
"""

import os

import toytree
import toyplot
import h5py
import numpy as np
import pandas as pd

from sklearn.manifold import TSNE
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.decomposition import NMF
from sklearn.model_selection import train_test_split
# from sklearn.metrics import confusion_matrix

from .utils import ABBA_IDX, BABA_IDX, AABB_IDX


"""
## Ideas 
1. Visualization and feature extraction: tSNE (base PCA or NMF)
2. Visualization of features: NMF or ExtraTrees.
3. Classification: ExtraTrees using raw or extracted features.
4. Classification + linear fit (e.g., supervised with edges and props.)

5. For a given true tree, we can map onto it which edges CAN be inferred and 
which are unlikely to be estimated correctly even with perfect data. 

6. 
"""


class Analysis(object):
    """
    ...

    Parameters
    ----------
    name: (str)
        Name of Database object used a prefix for database file.
    workdir: (str)
        Path name to directory where database files are located.
    scale: (int)
        0=None, 1=Normalize nxnx16x16 matrix, 2=normalize each 16x16 matrix.
    ulabel: (bool)
        Use undirected labels
    quiet: (bool)
        print progress
    seed: (int)
        random seed


    """
    def __init__(
        self,
        name, 
        workdir="./databases",
        scale=1, 
        ulabel=False, 
        transform=False,
        run=False,
        mask_admixture_min=0.01,
        mask_sisters=False,
        exclude_admixture_min=0.0,
        exclude_sisters=False,
        quiet=False,
        seed=None,
        ):

        # store arguments
        self.name = name
        self.workdir = os.path.abspath(os.path.expanduser(workdir))
        self.db_counts = os.path.join(self.workdir, self.name + ".counts.h5")
        self.db_labels = os.path.join(self.workdir, self.name + ".labels.h5")
        self.scale = scale
        self.ulabel = ulabel
        self.transform = transform
        self.quiet = quiet
        self.seed = seed
        self.mask_admixture_min = mask_admixture_min
        self.mask_sisters = mask_sisters
        self.exclude_admixture_min = exclude_admixture_min
        self.exclude_sisters = exclude_sisters
        assert os.path.exists(self.db_labels), (
            "path not found: {}".format(self.db_labels))
        assert os.path.exists(self.db_counts), (
            "path not found: {}".format(self.db_counts))

        # fill X and y with load_database(). X is the vector of data drawn from
        # counts as well as other possible features. y is the labels.
        self.tree = None
        self.counts = None
        self.df = None
        self.X = None
        self.y = None
        self.load_counts()
        self.load_labels()

        # mask or filter tests based on ...
        self.apply_masks()
        self.apply_filters()
        self.reshape_to_Xy()
        self.report()

        # transform data to reduce dimensionality / extract features
        self.add_babas_feature()
        # self.add_hils_feature()
        self.transform_data()

        # fill test/train with split dataset 
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.train_test_split()

        # train a supervised learning model
        self.model = None
        if run:
            self.train_model()



    def load_counts(self):
        """
        Load counts and labels Hdf5 databases generated by simcat.Database.
        """

        # load the snp counts and stack data
        with h5py.File(self.db_counts) as io5:

            # load the tree that was used in the simulations and the data
            self.tree = toytree.tree(io5.attrs["tree"])
            self.counts = io5["counts"][:]

            # [1] rescale to make counts proportional across ALL sims.
            # this seems to work much better than [2].
            if self.scale == 1:
                self.counts = self.counts / self.counts.max()


            # [2] rescale by make counts proportional across sims on same tree.
            if self.scale == 2:
                # iterate over matrixsets
                for i in range(self.counts.shape[0]):
                    # iterate over matrices
                    for j in range(self.counts.shape[1]):
                        # norm 16x16 matrices
                        self.counts[i, j] = (
                            self.counts[i, j] / self.counts[i, j].max()
                        )



    def load_labels(self):
        """
        Load counts and labels Hdf5 databases generated by simcat.Database.
        """
        # load the labels database
        with h5py.File(self.db_labels) as io5:
            df = pd.DataFrame({
                # "theta": io5["thetas"][:],
                "asource": io5["admixture"][:, 0].astype(int),
                "atarget": io5["admixture"][:, 1].astype(int),
                "aprop": io5["admixture"][:, 2],
            })

            # make a category for each (source, target) pair
            df["label"] = (
                df.asource.astype(str).str.cat(
                    df.atarget.astype(str), sep=",")
                )

            # unordered labels (no directionality)
            df["ulabel"] = [
                "{},{}".format(*i) for i in 
                df.label.apply(
                    lambda x: sorted([int(i) for i in x.split(",")])
                )
            ]

            # add a column to dataframe for sisters migration or not
            df["sisters"] = 0
            for idx in df.index:
                sis1 = df.asource[idx]
                sis2 = df.atarget[idx]
                node1 = self.tree.treenode.search_nodes(idx=sis1)[0]
                node2 = self.tree.treenode.search_nodes(idx=sis2)[0]
                if node1 in node2.get_sisters():
                    df.loc[idx, "sisters"] = 1

            # store to self
            self.df = df



    def report(self):
        """
        print a summary of the data
        """
        # print to screen
        if not self.quiet:
            print("Dataset: {}".format(self.name))
            print("loaded counts matrix: {}".format(self.counts.shape))
            if self.scale == 1:
                print("scaled integers to floats by max count")
            if self.scale == 2:
                print("scaled integers to floats by each quartet max count")
            print("reshaped into X: {}".format(self.X.shape))
            print("loaded labels DataFrame: {}".format(self.df.shape))
            print("subset as y: {}".format(self.y.shape))        



    def apply_masks(self):
        """
        Filter dataset to remove admixture edges between sisters or 
        edges with admixture proportions below cutoff.
        """
        # set NaN to labels below the cutoff
        if self.mask_admixture_min:
            self.df.loc[self.df.aprop < self.mask_admixture_min, "label"] = "NaN"
            self.df.loc[self.df.aprop < self.mask_admixture_min, "ulabel"] = "NaN"

        # exclude sisters
        if self.mask_sisters:
            self.df.loc[self.df.sisters, "label"] = "NaN"
            self.df.loc[self.df.sisters, "ulabel"] = "NaN"



    def apply_filters(self):
        """
        Filter dataset to remove admixture edges between sisters or 
        edges with admixture proportions below cutoff.
        """

        # get indices to keep
        if self.exclude_admixture_min:
            keepa = self.df.aprop >= self.exclude_admixture_min
        else:
            keepa = self.df.aprop >= -1

        if self.exclude_sisters:
            keepb = self.df.sisters == 0
        else:
            keepb = self.df.sisters > -1

        # drop tests
        keep = keepa.values & keepb.values
        self.df = self.df.loc[keep, :].reset_index(drop=True)
        self.counts = self.counts[keep, :]

        # report
        if not self.quiet:
            print("subset data to: {} tests".format(self.counts.shape[0]))



    def reshape_to_Xy(self):
        """
        Get X and y from the (maybe subsampled) data and reshape.
        """
        # flatten counts into X
        self.X = self.counts.reshape(self.counts.shape[0], -1)

        # select to use directed or undirected labels
        if self.ulabel:
            self.y = self.df.ulabel
        else:
            self.y = self.df.label



    def add_babas_feature(self):
        """
        calculate dstatistic for each matrix.
        """

        # empty arr for results
        babas = np.zeros((self.counts.shape[0], self.counts.shape[1]))

        # iterate over each test and matrix sets
        for a in range(babas.shape[0]):
            for b in range(babas.shape[1]):
                # this matrix
                mat = self.counts[a, b]

                # calculate
                abba = sum([mat[i] for i in ABBA_IDX])
                baba = sum([mat[i] for i in BABA_IDX])
                dstat = (abba - baba) / (abba + baba)

                # store result 
                babas[a, b] = dstat

        # append to the counts as additional feature
        self.X = np.concatenate([self.X, babas], axis=1)

        # report
        if not self.quiet:
            print("added {} features from abba-baba".format(babas.shape[1]))



    def add_hils_feature(self):
        """
        calculate hils statistic using aabb, abba, baba, aaab. This uses
        information about nsites and so should be applied to the full 
        counts data.

        TODO: this is incomplete. The final equation (13) is not yet finalized
        from the paper in the end here, and there is the problem of flipping
        the null hypothesis depending on whether p1 or p2 is the hybrid 
        taxon. Search "(below)" in the paper to find the relevant section.
        """

        # empty arr for results
        hils = np.zeros((self.counts.shape[0], self.counts.shape[1], 3))

        # iterate over each test and matrix sets
        for a in range(hils.shape[0]):
            for b in range(hils.shape[1]):
                # this matrix
                mat = self.counts[a, b]
                nsites = mat.sum()

                # calculate
                nabba = sum([mat[i] for i in ABBA_IDX])
                nbaba = sum([mat[i] for i in BABA_IDX])
                naabb = sum([mat[i] for i in AABB_IDX])
                abba = nabba / nsites
                baba = nbaba / nsites
                aabb = naabb / nsites

                f1 = aabb - baba
                f2 = abba - baba
                ratio = (f1 / f2)
                if f1 == f2:
                    hils[a, b] = 0, f1, f2
                    continue

                sigmaf1 = (
                    (1. / nsites) * sum([
                        aabb * (1. - aabb),
                        baba * (1. - baba), 
                        2. * aabb * baba
                        ])
                    )
                sigmaf2 = (
                    (1. / nsites) * sum([
                        abba * (1. - abba),
                        baba * (1. - baba),
                        2. * abba * baba,
                        ])
                    )

                covf1f2 = (
                    (1. / nsites) * sum([
                        abba * (1. - aabb),
                        aabb * baba,
                        abba * baba,
                        baba * (1. - baba)
                        ])
                    )

                numerator = (sigmaf2 * ratio) - sigmaf1
                p1 = sigmaf2 * ratio ** 2.
                p2 = 2 * covf1f2 * ratio + sigmaf1
                denominator = np.sqrt(p1 - p2)
                hils[a, b] = numerator / denominator

                num = f2 * ((f1 / f2) - 0.)
                p1 = (sigmaf2 * (f1 / f2)**2)
                p2 = ((2. * covf1f2 * (f1 / f2) + sigmaf1))
                denom = p1 - p2

                # calculate hils
                H = num / np.sqrt(abs(denom))
                hils[a, b] = H, f1, f2


        # append to the counts as additional feature
        # self.X = np.concatenate([self.X, babas], axis=1)

        # report
        if not self.quiet:
            print("added {} features from abba-baba".format(babas.shape[1]))



    def transform_data(self):
        """
        Use a dimensionality reduction method.
        """
        if not self.transform:
            return 

        # NMF n_components can be nfeatures-1 but slow
        if self.transform in ("nmf", "NMF"):
            if not self.quiet:
                print("training NMF model for data tranform")
            nmf = NMF(
                n_components=50, 
                init="random", 
                random_state=0)
            self.X = nmf.fit_transform(self.X)


        # TSNE perplexity and n-iter are important
        if self.transform in ("tsne", "TSNE", "t-SNE", "T-SNE"):
            if not self.quiet:
                print("training T-SNE model for data tranform")
            tsne = TSNE(
                n_components=2,
                perplexity=50,
                init="pca",
                n_iter=1000,
                random_state=0
            )
            self.X = tsne.fit_transform(self.X)



    def train_test_split(self, prop=0.33):
        "Use scikit to split dataset"

        # split data and labels
        a, b, c, d = train_test_split(
            self.X, 
            self.y, 
            test_size=prop,
            random_state=self.seed,
            )

        # store as attributes
        self.X_train = a
        self.X_test = b
        self.y_train = c
        self.y_test = d

        # print to screen
        if not self.quiet:
            print("split train/test data: {}/{}".format(
                self.X_train.shape, self.X_test.shape))



    def train_model(self, n_estimators=10000):
        """
        Train ML model...
        """
        if not self.quiet:
            print("training ExtraTrees model...")

        # select the model
        self.model = ExtraTreesClassifier(
            n_estimators=10000, 
            # max_features=np.sqrt(n_features).astype(int),
            n_jobs=4, 
            random_state=self.seed,
        )

        # fit the model to training data set
        self.model.fit(self.X_train, self.y_train)

        # print score to screen
        if not self.quiet:
            print("model score on training set: {:.3f}".format(
                self.model.score(self.X_train, self.y_train)
                ))
            print("model score on test/validation set: {:.3f}".format(
                self.model.score(self.X_test, self.y_test)
                ))




    # def _run(self, ipyclient, children=[]):
        """
        Generate new data quickly with ipcoal to fit with the trained model.
        """

    #     # load-balancer for single-threaded execution jobs
    #     lbview = ipyclient.load_balanced_view()

    #     # get all edges on the tree
    #     edges = get_all_admix_edges(self.tree)

    #     # other params to iterate over
    #     aprops = np.arange(0.05, 0.30, 0.05)
    #     nsnps = np.arange(5000, 55000, 5000)
    #     reps = np.arange(10)

    #     # total number of samples
    #     ns = len(edges) * aprops.size * nsnps.size * reps.size

    #     # build empty data frame
    #     validate = pd.DataFrame({
    #         "admix_edge": [0] * ns,
    #         "admix_prop": 0,
    #         "nsnps": 0,
    #         "est": 0,
    #         "prob": 0,
    #     }, columns=["nsnps", "admix_edge", "admix_prop", "est", "prob"],
    #     )

    #     # dataframe index 
    #     idx = 0
    #     rasyncs = {}

    #     # simulate a 
    #     for edge in edges:
    #         for prop in aprops:
    #             for snp in nsnps:
    #                 for rep in reps:

    #                     # submit jobs
    #                     kwargs = {
    #                         'tree': self.tree, 
    #                         'admixture_edges': [(edge[0], edge[1], 0.5, prop)],
    #                         'nsnps': snp,
    #                         'run': True,
    #                     }
    #                     rasyncs[idx] = lbview.apply(Model, **kwargs)

    #                     # fill labels
    #                     validate.loc[idx, :3] = snp, edge, prop


    #                     # edgestr = str(edge).replace(" ", "")[1:-1]
    #                     # testX = test.counts.flatten().reshape((1, -1))
    #                     # est = self.model.predict(testX)
    #                     # probs = self.model.predict_proba(testX)[0]
    #                     # tidx = list(self.model.classes_).index(edgestr)
    #                     # prob = probs[tidx]
    #                     # validate.loc[idx] = rep, nsnps, edgestr, magn, est, prob
    #                     idx += 1


    # def run(self, force=True, ipyclient=None, show_cluster=False, auto=False):
    #     """
    #     Build a DataFrame of predictive accuracy scores for a trained model
    #     when new data is simulated on it under a range of input settings.

    #     Parameters:
    #     -----------
    #     ipyclient (ipyparallel.Client object):
    #         A connected ipyclient object. If ipcluster instance is 
    #         not running on the default profile then ...
    #     """
    #     # distribute filling jobs in parallel
    #     pool = Parallel(
    #         tool=self,
    #         rkwargs={},
    #         ipyclient=ipyclient,
    #         show_cluster=show_cluster,
    #         auto=auto,
    #         )
    #     pool.wrap_run()



    def tsne_plot(self):


        # build color vector
        colors = [0] * self.df.shape[0]
        for idx in self.df.index:
            if self.df.sisters[idx]:
                colors[idx] = 1
            if self.df.asource[idx] == 5:
                colors[idx] = 2
            if self.df.asource[idx] == 4:
                colors[idx] = 3
        colors = [toyplot.color.Palette()[i] for i in colors]


        # save a 3D plot if 3-dimension...
        # TODO w/ ipyvolume.