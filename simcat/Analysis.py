#!/usr/bin/env python

"""
Machine learning code for analyzing database info...
"""

import os
import itertools
import toytree
import toyplot
import h5py
import numpy as np
import pandas as pd

from sklearn.manifold import TSNE
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.decomposition import NMF, PCA
from sklearn.model_selection import train_test_split
# from sklearn.metrics import confusion_matrix

# try to import multicore-TSNE if available clobbers the scikit version
try:
    from MulticoreTSNE import MulticoreTSNE as TSNE
except ImportError:
    pass

from .utils import calculate_dstat, calculate_hils_f12, calculate_simple_f12


"""
## Ideas 
4. Classification + linear fit (e.g., supervised with edges and props.)

5. For a given true tree, we can map onto it which edges CAN be inferred and 
which are unlikely to be estimated correctly even with perfect data. 

6. 
"""


class Analysis(object):
    """
    ...

    Parameters
    ----------
    name: (str)
        Name of Database object used a prefix for database file.
    workdir: (str)
        Path name to directory where database files are located.
    scale: (int)
        0=None, 1=Normalize nxnx16x16 matrix, 2=normalize each 16x16 matrix.
    ulabel: (bool)
        Use undirected labels
    quiet: (bool)
        print progress
    seed: (int)
        random seed
    outgroup: (str or int)
        The idx label(s) of an outgroup clade. If setting this option it is 
        assumed this clade is not involved in admixture. The SNPs count data 
        is still used for this clade when inferring all other admixture edges
        but simulations that include admixture edges to or from this taxon 
        are removed from the training and test data.
    features: list
        Compute additional featues on the count data and add to the flattened
        feature vector (X). Current options include ["dstat", "f12",]
    """
    def __init__(
        self,
        name, 
        workdir="./databases",
        scale=1, 
        ulabel=False, 
        transform=False,
        run=False,
        mask_admixture_min=0.01,
        mask_sisters=False,
        exclude_admixture_min=0.0,
        exclude_sisters=False,
        outgroup=None,
        features=None,
        quiet=False,
        seed=None,
        ):

        # store arguments
        self.name = name
        self.workdir = os.path.abspath(os.path.expanduser(workdir))
        self.db_counts = os.path.join(self.workdir, self.name + ".counts.h5")
        self.db_labels = os.path.join(self.workdir, self.name + ".labels.h5")
        self.scale = scale
        self.ulabel = ulabel
        self._transform = transform
        self.quiet = quiet
        self.seed = seed
        self.mask_admixture_min = mask_admixture_min
        self.mask_sisters = mask_sisters
        self.exclude_admixture_min = exclude_admixture_min
        self.exclude_sisters = exclude_sisters
        self.outgroup = outgroup
        self.features = (features if features else [])
        assert os.path.exists(self.db_labels), (
            "path not found: {}".format(self.db_labels))
        assert os.path.exists(self.db_counts), (
            "path not found: {}".format(self.db_counts))

        # fill X and y with load_database(). X is the vector of data drawn from
        # counts as well as other possible features. y is the labels.
        self.tree = None
        self.counts = None
        self.df = None
        self.X = None
        self.y = None

        print("[init] {}".format(self.name))
        self.load_counts()
        self.load_labels()

        # mask or filter tests based on ...
        self.apply_masks()
        self.apply_filters()
        self.reshape_to_Xy()

        # transform data to reduce dimensionality / extract features
        self.add_features()
        self.transform_data()

        # fill test/train with split dataset 
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.train_test_split()

        # train a supervised learning model
        self.model = None
        if run:
            self.train_model()



    def load_counts(self):
        """
        Load counts and labels Hdf5 databases generated by simcat.Database.
        """

        # load the snp counts and stack data
        with h5py.File(self.db_counts, 'r') as io5:

            # load the tree that was used in the simulations and the data
            self.tree = toytree.tree(io5.attrs["tree"])
            self.counts = io5["counts"][:]
            self.svds = io5["svds"][:]

            # [1] rescale to make counts proportional across ALL sims.
            # this seems to work much better than [2].
            if self.scale == 1:
                self.counts = self.counts / self.counts.max()
                self.svds = self.svds / self.svds.max()

            # [2] rescale by make counts proportional across sims on same tree.
            if self.scale == 2:
                # iterate over matrixsets
                for i in range(self.counts.shape[0]):
                    # iterate over matrices
                    for j in range(self.counts.shape[1]):
                        # norm 16x16 matrices
                        self.counts[i, j] = (
                            self.counts[i, j] / self.counts[i, j].max()
                        )
            if not self.quiet:
                print("[load] {}".format(self.counts.shape))



    def load_labels(self):
        """
        Load counts and labels Hdf5 databases generated by simcat.Database.
        """
        # load the labels database
        with h5py.File(self.db_labels, 'r') as io5:
            df = pd.DataFrame({
                # "theta": io5["thetas"][:],
                "asource": io5["admixture"][:, 0].astype(int),
                "atarget": io5["admixture"][:, 1].astype(int),
                "aprop": io5["admixture"][:, 2],
            })

            # make a category for each (source, target) pair
            df["label"] = (
                df.asource.astype(str).str.cat(
                    df.atarget.astype(str), sep=",")
                )

            # unordered labels (no directionality)
            df["ulabel"] = [
                "{},{}".format(*i) for i in 
                df.label.apply(
                    lambda x: sorted([int(i) for i in x.split(",")])
                )
            ]

            # add a column to dataframe for sisters migration or not
            df["sisters"] = 0
            for idx in df.index:
                sis1 = df.asource[idx]
                sis2 = df.atarget[idx]
                node1 = self.tree.treenode.search_nodes(idx=sis1)[0]
                node2 = self.tree.treenode.search_nodes(idx=sis2)[0]
                if node1 in node2.get_sisters():
                    df.loc[idx, "sisters"] = 1

            # store to self
            self.df = df



    def apply_masks(self):
        """
        Filter dataset to remove admixture edges between sisters or 
        edges with admixture proportions below cutoff.
        """
        # set NaN to labels below the cutoff
        if self.mask_admixture_min:
            self.df.loc[self.df.aprop < self.mask_admixture_min, "label"] = "NaN"
            self.df.loc[self.df.aprop < self.mask_admixture_min, "ulabel"] = "NaN"

        # exclude sisters
        if self.mask_sisters:
            self.df.loc[self.df.sisters, "label"] = "NaN"
            self.df.loc[self.df.sisters, "ulabel"] = "NaN"



    def apply_filters(self):
        """
        Filter dataset to remove admixture edges between sisters or 
        edges with admixture proportions below cutoff.
        """

        # get indices to keep
        if self.exclude_admixture_min:
            keepa = self.df.aprop >= self.exclude_admixture_min
        else:
            keepa = self.df.aprop >= -1

        if self.exclude_sisters:
            keepb = self.df.sisters == 0
        else:
            keepb = self.df.sisters > -1

        if self.outgroup:
            keepc = self.df.label.apply(lambda x: self.outgroup not in x)
        else:
            keepc = self.df.sisters > -1

        # drop tests
        keep = keepa.values & keepb.values & keepc.values
        self.df = self.df.loc[keep, :].reset_index(drop=True)
        self.counts = self.counts[keep, :]

        # report
        if not self.quiet:
            print("[filter] {}".format(self.counts.shape))



    def reshape_to_Xy(self):
        """
        Get X and y from the (maybe subsampled) data and reshape.
        """
        # flatten counts into X
        self.X = self.counts.reshape(self.counts.shape[0], -1)

        # select to use directed or undirected labels
        if self.ulabel:
            self.y = self.df.ulabel
        else:
            self.y = self.df.label
        print("[vectorize] {}".format(self.X.shape))


    def add_features(self):
        if "dstat" in self.features:
            self.add_dstat_feature()
        if "f12" in self.features:
            self.add_simple_f12()
        if "h12" in self.features:
            self.add_hils12_feature()
        if "svd" in self.features:
            self.add_svd_feature()
        if "pca" in self.features:
            self.add_pca_feature()
        if "tsne" in self.features:
            self.add_tsne_feature()
        # if "counts" in self.features:
            # self.add_counts()


    def add_svd_feature(self):

        # empty arr for results
        svds = np.zeros((self.counts.shape[0], self.counts.shape[1], 16))

        # iterate over each test and matrix sets
        for test in range(svds.shape[0]):
            for qset in range(self.counts.shape[1]):
                # this matrix
                mat = self.counts[test, qset]
                u, s, v = np.linalg.svd(mat)
                svds[test, qset] = s

        # append to the counts as additional feature
        self.X = np.concatenate(
            [self.X, svds.reshape(self.X.shape[0], -1)],
            axis=1,
        )

        # report
        if not self.quiet:
            print("added {} features (svds)".format(svds.shape))



    def add_dstat_feature(self):
        """
        calculate dstatistic for each matrix.
        """
        # empty arr for results
        dstats = np.zeros((self.counts.shape[0], self.counts.shape[1]))

        # iterate over each test and matrix sets
        for test in range(dstats.shape[0]):
            for qset in range(dstats.shape[1]):
                # this matrix
                mat = self.counts[test, qset]
                dstats[test, qset] = calculate_dstat(mat)

        # append to the counts as additional feature
        self.X = np.concatenate([self.X, dstats], axis=1)

        # report
        print(
            "[feature extraction] added {} features (dstat)"
            .format(dstats.shape[1])
        )


    def add_simple_f12(self):
        # empty arr for results
        hils12 = np.zeros((self.counts.shape[0], self.counts.shape[1]))

        # iterate over each test and matrix sets
        for test in range(hils12.shape[0]):
            for qset in range(hils12.shape[1]):
                # this matrix
                mat = self.counts[test, qset]
                hils12[test, qset] = calculate_simple_f12(mat)

        # append to the counts as additional feature
        self.X = np.concatenate([self.X, hils12], axis=1)

        # report
        if not self.quiet:
            print("added {} features (Hils f1/f2)".format(hils12.shape[1]))



    def add_hils12_feature(self):
        """
        calculate hils statistic using aabb, abba, baba, aaab. This uses
        information about nsites and so should be applied to the full 
        counts data.
        """
        # empty arr for results
        hils12 = np.zeros((self.counts.shape[0], self.counts.shape[1]))

        # iterate over each test and matrix sets
        for test in range(hils12.shape[0]):
            for qset in range(hils12.shape[1]):
                # this matrix
                mat = self.counts[test, qset]
                hils12[test, qset] = calculate_hils_f12(mat)

        # append to the counts as additional feature
        self.X = np.concatenate([self.X, hils12], axis=1)

        # report
        if not self.quiet:
            print("added {} features (Hils f1/f2)".format(hils12.shape[1]))



    def add_pca_feature(self, n_components=10):
        """
        Add PC axes loadings as new features
        """
        loadings = PCA(n_components=n_components).fit_transform(self.X)
        self.X = np.concatenate([self.X, loadings], axis=1)
        if not self.quiet:
            print(
                "[feature extraction] added {} features (PCA)"
                .format(loadings.shape[1])
            )



    def add_tsne_feature(self, perplexity=50):
        """
        Add PC axes loadings as new features
        """
        tsne = TSNE(
            n_components=2,
            perplexity=50,
            n_iter=1000,
            random_state=0
        )
        Xt = tsne.fit_transform(self.X)
        self.X = np.concatenate([self.X, Xt], axis=1)
        if not self.quiet:
            print(
                "[feature extraction] added {} features (t-SNE)"
                .format(Xt.shape[1])
            )



    def transform_data(self, model=None):
        """
        Use a dimensionality reduction method to update .Xt
        """
        # get transform method
        transformer = (model if model else self._transform)
        if not transformer:
            self.Xt = self.X
            return

        # clean method name
        transformer = transformer.lower().replace("-", "")

        # NMF n_components can be nfeatures-1 but slow
        if transformer == "nmf":
            if not self.quiet:
                print("transforming data X with NMF to Xt")
            nmf = NMF(
                n_components=50, 
                init="random", 
                random_state=0)
            self.Xt = nmf.fit_transform(self.X)


        # TSNE perplexity and n-iter are important
        if transformer == "tsne":
            if not self.quiet:
                print("transforming data X with T-SNE to Xt")
            tsne = TSNE(
                n_components=2,
                perplexity=50,
                n_iter=1000,
                random_state=0
            )
            self.Xt = tsne.fit_transform(self.X)


        if transformer == "pca":
            self.Xt = PCA().fit_transform(self.X)



    def train_test_split(self, prop=0.33):
        "Use scikit to split dataset"

        # split data and labels
        a, b, c, d = train_test_split(
            self.Xt, 
            self.y, 
            test_size=prop,
            random_state=self.seed,
            )

        # store as attributes
        self.X_train = a
        self.X_test = b
        self.y_train = c
        self.y_test = d

        # print to screen
        if not self.quiet:
            print("[train/test] {}/{}".format(
                self.X_train.shape, self.X_test.shape))



    def train_model(self, n_estimators=10000):
        """
        Train ML model...
        """
        if not self.quiet:
            print("training ExtraTrees model...")

        # select the model
        self.model = ExtraTreesClassifier(
            n_estimators=10000, 
            n_jobs=4, 
            random_state=self.seed,
        )

        # fit the model to training data set
        self.model.fit(self.X_train, self.y_train)

        # print score to screen
        if not self.quiet:
            print("model score on training set: {:.3f}".format(
                self.model.score(self.X_train, self.y_train)
                ))
            print("model score on test/validation set: {:.3f}".format(
                self.model.score(self.X_test, self.y_test)
                ))




    # def _run(self, ipyclient, children=[]):
        """
        Generate new data quickly with ipcoal to fit with the trained model.
        """

    #     # load-balancer for single-threaded execution jobs
    #     lbview = ipyclient.load_balanced_view()

    #     # get all edges on the tree
    #     edges = get_all_admix_edges(self.tree)

    #     # other params to iterate over
    #     aprops = np.arange(0.05, 0.30, 0.05)
    #     nsnps = np.arange(5000, 55000, 5000)
    #     reps = np.arange(10)

    #     # total number of samples
    #     ns = len(edges) * aprops.size * nsnps.size * reps.size

    #     # build empty data frame
    #     validate = pd.DataFrame({
    #         "admix_edge": [0] * ns,
    #         "admix_prop": 0,
    #         "nsnps": 0,
    #         "est": 0,
    #         "prob": 0,
    #     }, columns=["nsnps", "admix_edge", "admix_prop", "est", "prob"],
    #     )

    #     # dataframe index 
    #     idx = 0
    #     rasyncs = {}

    #     # simulate a 
    #     for edge in edges:
    #         for prop in aprops:
    #             for snp in nsnps:
    #                 for rep in reps:

    #                     # submit jobs
    #                     kwargs = {
    #                         'tree': self.tree, 
    #                         'admixture_edges': [(edge[0], edge[1], 0.5, prop)],
    #                         'nsnps': snp,
    #                         'run': True,
    #                     }
    #                     rasyncs[idx] = lbview.apply(Model, **kwargs)

    #                     # fill labels
    #                     validate.loc[idx, :3] = snp, edge, prop


    #                     # edgestr = str(edge).replace(" ", "")[1:-1]
    #                     # testX = test.counts.flatten().reshape((1, -1))
    #                     # est = self.model.predict(testX)
    #                     # probs = self.model.predict_proba(testX)[0]
    #                     # tidx = list(self.model.classes_).index(edgestr)
    #                     # prob = probs[tidx]
    #                     # validate.loc[idx] = rep, nsnps, edgestr, magn, est, prob
    #                     idx += 1


    # def run(self, force=True, ipyclient=None, show_cluster=False, auto=False):
    #     """
    #     Build a DataFrame of predictive accuracy scores for a trained model
    #     when new data is simulated on it under a range of input settings.

    #     Parameters:
    #     -----------
    #     ipyclient (ipyparallel.Client object):
    #         A connected ipyclient object. If ipcluster instance is 
    #         not running on the default profile then ...
    #     """
    #     # distribute filling jobs in parallel
    #     pool = Parallel(
    #         tool=self,
    #         rkwargs={},
    #         ipyclient=ipyclient,
    #         show_cluster=show_cluster,
    #         auto=auto,
    #         )
    #     pool.wrap_run()



    def draw_tsne_plot(self, perplexity=50, n_iter=10000, random_state=None):
        """
        Draw 2-D plot of t-SNE transformed data (Xt). 
        """
        # get transformed data
        tsne = TSNE(
            n_components=2,
            perplexity=perplexity,
            n_iter=n_iter,
            random_state=random_state,
            n_jobs=4,
        )
        pca = PCA()
        self.Xt = tsne.fit_transform(pca.fit_transform(self.X))
        self._draw_scatter(self.Xt[:, 0], self.Xt[:, 1])



    def _draw_scatter(self, dataX, dataY):
        """
        2-D scatterplot...
        """
        # iterate through colors and shapes
        colors = toyplot.color.brewer.palette("Paired")
        colorshapes = itertools.product(["o", "s", "d", "v"], colors)

        # color dictionary applied grey to NaN (masked) labels
        cdict = {
            "NaN": {
                "fill": 'rgba(74.1%,74.1%,74.1%,1.000)', 
                'stroke': 'none',
            },
        }

        # apply same color to fill or stroke for edges in each direction
        labels = set(self.df.ulabel[self.df.sisters == 0])
        labels = labels - {"NaN"}
        for label, colorshape in zip(labels, colorshapes):

            shape, color = colorshape

            # set ->
            cdict[label] = {
                "marker": shape,
                "stroke": "none",
                "fill": toyplot.color.to_css(color),
            }

            # set <-
            alt = "{1},{0}".format(*label.split(","))
            cdict[alt] = {
                "shape": shape,
                "stroke": toyplot.color.to_css(color),
                "stroke-width": 1.5,
                "fill": 'rgba(100%,100%,100%,1.000)',
            }

        # generate markers for each test
        markers = []
        for idx in self.df.index:
            if self.df.sisters[idx]:
                mark = toyplot.marker.create(
                    shape="o",
                    size=3,  # + (ml.df.aprop[idx] / ml.df.aprop.max()) * 10,# * 35,
                    mstyle=cdict["NaN"])

            else:
                mark = toyplot.marker.create(
                    shape='o',
                    size=3 + (self.df.aprop[idx] / self.df.aprop.max()) * 10,# * 35,
                    mstyle=cdict[self.df.label[idx]],
                )
            markers.append(mark)


        canvas, axes, mark = toyplot.scatterplot(
            dataX,
            dataY,
            width=400, height=400,
            marker=markers,
            title=self.df.label,
        )
        axes.x.label.text = "t-SNE axis 1"
        axes.y.label.text = "t-SNE axis 2"
        return canvas, axes, mark
